{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMS0Ezn9N0li",
        "outputId": "c21fc0fa-b34a-4304-81f3-6ae71ef7e915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 227.2499\n",
            "Epoch 10, Loss: 157.0853\n",
            "Epoch 20, Loss: 100.3892\n",
            "Epoch 30, Loss: 58.1377\n",
            "Epoch 40, Loss: 32.8383\n",
            "\n",
            "Raw text:  We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
            "\n",
            "Context: ['People', 'create', 'to', 'direct']\n",
            "Prediction: programs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Helper function to convert words to tensor indices\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Parameters\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Sample raw text\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "# Vocabulary creation\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix: word for ix, word in enumerate(vocab)}\n",
        "\n",
        "# Generate training data (context, target)\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "# Define CBOW Model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_embedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1, -1)\n",
        "\n",
        "# Create model, loss function, optimizer\n",
        "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------\n",
        "# TRAINING LOOP\n",
        "# -------------------------\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_ix)\n",
        "        log_probs = model(context_vector)\n",
        "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss.item():.4f}')\n",
        "\n",
        "# -------------------------\n",
        "# TESTING\n",
        "# -------------------------\n",
        "context = ['People', 'create', 'to', 'direct']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)\n",
        "\n",
        "print(\"\\nRaw text: \", \" \".join(raw_text))\n",
        "print(\"\\nContext:\", context)\n",
        "print(f\"Prediction: {ix_to_word[torch.argmax(a[0]).item()]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "903qRq2aOHTP"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}